{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# other\n",
    "import time\n",
    "\n",
    "# sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILEPATH = \"../data/all_hourly_data.h5\"\n",
    "patients = pd.read_hdf(DATA_FILEPATH, \"patients\")\n",
    "vitals_labs_mean = pd.read_hdf(DATA_FILEPATH, \"vitals_labs_mean\")\n",
    "interventions = pd.read_hdf(DATA_FILEPATH, \"interventions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# true positive rate\n",
    "# false positive rate\n",
    "# equal opportunity\n",
    "# interpretability\n",
    "\n",
    "# candidates:\n",
    "# 1. decision tree\n",
    "# 2. dense neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Task Formulation: predict whether a patient will die, given the first 24 hours of their stay\n",
    "\"\"\"\n",
    "\n",
    "# SETTINGS\n",
    "window_size = 24  # the first WINDOW_SIZE hours of the patient's stay\n",
    "gap_time = 6  # the number of hours the patient lived at least after the first WINDOW_SIZE hours (to avoid label leakage, see MIMIC-III Extract paper)\n",
    "test_size = 0.2  # proportion of the data that wil lbe used for testing\n",
    "val_size = 0.125  # proportion of the training data that will be used for validation\n",
    "random_state = 42  # random state is used to set a seed for randomness, which is only relevant for reproducibility purposes\n",
    "max_missing = 0.8  # maximum percentage of missing values after forward fill for a measurement to be dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 14.19s\n",
      "Train set: 24129 rows, 1879 columns\n",
      "Validation set: 3448 rows, 1879 columns\n",
      "Test set: 6895 rows, 1879 columns\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "\"\"\"PREPARE VITALS LABES AND INTERVENTIONS\"\"\"\n",
    "# get target variable (died in ICU) of patients that stayed at least GAP_TIME + WINDOW_SIZE hours in the ICU\n",
    "y = patients[\"mort_icu\"]\n",
    "\n",
    "# select first 24 hours of patient data for ICU stays of selected patients\n",
    "X = vitals_labs_mean.droplevel(\n",
    "    level=\"Aggregation Function\", axis=1\n",
    ").copy()  # drop constant 'mean' level\n",
    "X = X.join(interventions)  # add interventions\n",
    "X = X[\n",
    "    (\n",
    "        X.index.get_level_values(\"icustay_id\").isin(\n",
    "            set(y.index.get_level_values(\"icustay_id\"))\n",
    "        )\n",
    "    )\n",
    "    & (X.index.get_level_values(\"hours_in\") < 24)\n",
    "]\n",
    "\n",
    "# reset index so only subject_id is in index\n",
    "y = y.reset_index(level=[\"hadm_id\", \"icustay_id\"], drop=True)\n",
    "X = X.reset_index(level=[\"hadm_id\", \"icustay_id\"], drop=True).unstack()\n",
    "\n",
    "X = X.groupby(level=[\"subject_id\"]).cumsum()\n",
    "\n",
    "\n",
    "\"\"\"SPLIT DATA\"\"\"\n",
    "# define train/test split based on index\n",
    "X_train_r, X_test_r, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=test_size, random_state=random_state\n",
    ")\n",
    "X_train_r, X_val_r, y_train, y_val = train_test_split(\n",
    "    X_train_r, y_train, test_size=val_size, random_state=random_state\n",
    ")\n",
    "\n",
    "\"\"\"IMPUTE MISSING VALUES\"\"\"\n",
    "# compute mean and std of raw values\n",
    "mean = X_train_r.stack().mean(axis=0)\n",
    "std = X_train_r.stack().std(axis=0)\n",
    "\n",
    "# forward fill missing values\n",
    "X_train = X_train_r.copy().stack(dropna=False).groupby(\"subject_id\").ffill()\n",
    "X_test = X_test_r.copy().stack(dropna=False).groupby(\"subject_id\").ffill()\n",
    "X_val = X_val_r.copy().stack(dropna=False).groupby(\"subject_id\").ffill()\n",
    "\n",
    "# remove features with too many missing values\n",
    "missing = X_train.isna().sum() > max_missing * len(X_train)\n",
    "missing = missing[missing].index\n",
    "X_train = X_train.drop(missing, axis=1)\n",
    "X_val = X_val.drop(missing, axis=1)\n",
    "X_test = X_test.drop(missing, axis=1)\n",
    "\n",
    "# impute remaining missing values with the mean of original data and get to hourly\n",
    "means = X_train_r.stack().mean(\n",
    "    axis=0\n",
    ")  # get average of hourly measurement value for each feature\n",
    "X_train = X_train.fillna(means).unstack()\n",
    "X_test = X_test.fillna(means).unstack()\n",
    "X_val = X_val.fillna(means).unstack()\n",
    "\n",
    "\"\"\"ADD DEMOGRAPHICS\"\"\"\n",
    "# convert gender to boolean feature\n",
    "demo = pd.get_dummies(patients[[\"gender\"]], drop_first=True)\n",
    "\n",
    "# replace age of people older than 89 by 90\n",
    "demo[\"age\"] = patients[\"age\"]\n",
    "demo.loc[demo[demo[\"age\"] >= 90].index, \"age\"] = 90\n",
    "\n",
    "# create ethnicity columns\n",
    "demo[\"white\"] = patients[\"ethnicity\"].str.contains(\"WHITE\")\n",
    "demo[\"black\"] = patients[\"ethnicity\"].str.contains(\"BLACK\")\n",
    "demo[\"asian\"] = patients[\"ethnicity\"].str.contains(\"ASIAN\")\n",
    "demo[\"hispanic\"] = patients[\"ethnicity\"].str.contains(\"HISPANIC\")\n",
    "demo[\"other/unknown\"] = ~(\n",
    "    demo[\"white\"] | demo[\"black\"] | demo[\"asian\"] | demo[\"hispanic\"]\n",
    ")\n",
    "\n",
    "# reset index to remove icu id and hadm id\n",
    "demo = demo.reset_index(level=[1, 2], drop=True)\n",
    "demo.columns = pd.MultiIndex.from_product([demo.columns, [-1]])\n",
    "\n",
    "# add demographics to dataframes\n",
    "X_train = X_train.join(demo)\n",
    "X_val = X_val.join(demo)\n",
    "X_test = X_test.join(demo)\n",
    "\n",
    "# combine column levels\n",
    "X_train.columns = X_train.columns.map('{0[0]}|{0[1]}'.format)\n",
    "X_val.columns = X_val.columns.map('{0[0]}|{0[1]}'.format)\n",
    "X_test.columns = X_test.columns.map('{0[0]}|{0[1]}'.format)\n",
    "\n",
    "X_train[X_train.select_dtypes(bool).columns] = X_train.select_dtypes(bool).astype(np.float64)\n",
    "X_val[X_val.select_dtypes(bool).columns] = X_val.select_dtypes(bool).astype(np.float64)\n",
    "X_test[X_test.select_dtypes(bool).columns] = X_test.select_dtypes(bool).astype(np.float64)\n",
    "\n",
    "\"\"\"RESET INDEX\"\"\"\n",
    "y_train = y_train.reindex(X_train.index)\n",
    "y_test = y_test.reindex(X_test.index)\n",
    "y_val = y_val.reindex(X_val.index)\n",
    "\n",
    "\"\"\"PRINT STATS\"\"\"\n",
    "print(\"Time: %.2fs\" % (time.time() - start_time))\n",
    "print(\"Train set: %s rows, %s columns\" % X_train.shape)\n",
    "print(\"Validation set: %s rows, %s columns\" % X_val.shape)\n",
    "print(\"Test set: %s rows, %s columns\" % X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unskew the data\n",
    "X_train_model = X_train.copy()\n",
    "for col in X_train.columns:\n",
    "    X_train_model[col] = (X_train_model[col] - X_train_model[col].mean()) / X_train_model[col].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TP(y_true, y_pred):\n",
    "    return np.sum(np.logical_and(y_true == 1.0, y_pred == 1.0))\n",
    "\n",
    "def FP(y_true, y_pred):\n",
    "    return np.sum(np.logical_and(y_true == 0.0, y_pred == 1.0))\n",
    "\n",
    "def FN(y_true, y_pred):\n",
    "    return np.sum(np.logical_and(y_true == 1.0, y_pred == 0.0))\n",
    "\n",
    "def TN(y_true, y_pred):\n",
    "    return np.sum(np.logical_and(y_true == 0.0, y_pred == 0.0))\n",
    "\n",
    "def TPR(y_true, y_pred):\n",
    "    return TP(y_true, y_pred)/(TP(y_true, y_pred) + FN(y_true, y_pred))\n",
    "\n",
    "def FPR(y_true, y_pred):\n",
    "    return FP(y_true, y_pred)/(FP(y_true, y_pred) + TN(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nDecisionTreeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15760/1496928093.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdecision_tree_classifier_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdecision_tree_classifier_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecision_tree_classifier_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\Programs\\Python\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    967\u001b[0m         \"\"\"\n\u001b[0;32m    968\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 969\u001b[1;33m         super().fit(\n\u001b[0m\u001b[0;32m    970\u001b[0m             \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    971\u001b[0m             \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programs\\Python\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    170\u001b[0m             \u001b[0mcheck_X_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    171\u001b[0m             \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 172\u001b[1;33m             X, y = self._validate_data(\n\u001b[0m\u001b[0;32m    173\u001b[0m                 \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_separately\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    174\u001b[0m             )\n",
      "\u001b[1;32md:\\Programs\\Python\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    589\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"estimator\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_X_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m                     \u001b[0mcheck_X_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m                 \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"X\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_X_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    592\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m\"estimator\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcheck_y_params\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m                     \u001b[0mcheck_y_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mdefault_check_params\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcheck_y_params\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programs\\Python\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    897\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    898\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 899\u001b[1;33m             _assert_all_finite(\n\u001b[0m\u001b[0;32m    900\u001b[0m                 \u001b[0marray\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    901\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\Programs\\Python\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[0;32m    144\u001b[0m                     \u001b[1;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m                 )\n\u001b[1;32m--> 146\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input X contains NaN.\nDecisionTreeClassifier does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "decision_tree_classifier_model = DecisionTreeClassifier()\n",
    "decision_tree_classifier_model = decision_tree_classifier_model.fit(X_train_model, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positives count: 191\n",
      "False positives count: 312\n",
      "False negatives count: 247\n",
      "True negatives count: 6145\n",
      "True positive rate: 0.4360730593607306\n",
      "False positive rate: 0.04831965308967012\n"
     ]
    }
   ],
   "source": [
    "y_true = y_test.to_numpy()\n",
    "y_pred = np.round(decision_tree_classifier_model.predict(X_test))\n",
    "\n",
    "print(\"True positives count: \" + str(TP(y_true, y_pred)))\n",
    "print(\"False positives count: \" + str(FP(y_true, y_pred)))\n",
    "print(\"False negatives count: \" + str(FN(y_true, y_pred)))\n",
    "print(\"True negatives count: \" + str(TN(y_true, y_pred)))\n",
    "print(\"True positive rate: \" + str(TPR(y_true, y_pred)))\n",
    "print(\"False positive rate: \" + str(FPR(y_true, y_pred)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5312a05d4c1062e7e99dc4a1327331a73c17135ec320657c0e8e8843948bfa85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
